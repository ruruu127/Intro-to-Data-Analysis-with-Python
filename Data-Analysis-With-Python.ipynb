{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis with Python\n",
    "Python is a **popular** programming language for **data analysis**. There are many modules and functions available for analysis purposes in Python. These can be used to build **machine learning models** to **classify** data or make **predictions**.\n",
    "\n",
    "We are going to work on the Iris species dataset and do some data analysis using Python. The Iris dataset contains input / independent variables such as the sepal width and length, and petal width and length. There is also the dependent / target variable, which is the name of the Iris species.\n",
    "\n",
    "|<img src='images/Irissetosa1.jpg' width=300 />|<img src='images/Irisversicolor.jpg' width=300 />|<img src='images/Iris_virginica_2.jpg' width=300 />|\n",
    "|:--:|:--:|:--:|\n",
    "|*Iris setosa*|*Iris versicolor*|*Iris virginica*|\n",
    "\n",
    "We are going to use some libraries or modules that are generally used in data analysis, which are `pandas`, `matplotlib`, `seaborn` and `scikit-learn`.\n",
    "\n",
    "|<img src='images/pandas.png' width=250 />|<img src='images/matplotlib.svg' width=250 />|<img src='images/seaborn.svg' width=250 />|<img src='images/scikit_learn.png' width=250 />|\n",
    "|:--:|:--:|:--:|:--:|\n",
    "|`pandas`|`matplotlib`|`seaborn`|`scikit-learn`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries\n",
    "The first step is to load the libraries into the workspace.\n",
    "+ `pandas` is the most popular library where it is used for **dataset or dataframe manipulation**.\n",
    "\t+ From the `pandas` module, we are going to import a function called `scatter_matrix()`, which we will use to check on the **correlation** of our dataset.\n",
    "+ The `matplotlib` and `seaborn` modules are used for **data visualization**.\n",
    "+ From the `scikit-learn` module, we are going to import:\n",
    "\t+ `train_test_split()` from `model_selection` submodule: to **split our data into training and testing sets**.\n",
    "\t+ `LogisticRegression()` from `linear_model` submodule: to **create a logistic regression model**.\n",
    "\t+ `accuracy_score`, `confusion_matrix` and `classification_report` from `metrics` submodule: to check the **accuracy** of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import dataset\n",
    "As mentioned, we will be working on the *Iris* species dataset. We are going to load the dataset into our workspace using the `read_csv()` function from `pandas`. We save this into a variable named `iris`.\n",
    "\n",
    "***Note: Remember to put the dataset in the same working directory as this notebook. If you're not sure what your current working directory is and would like to change it, use the following code:***\n",
    "\n",
    "```python\n",
    "# Check current working directory\n",
    "%pwd\n",
    "\n",
    "# Change working directory\n",
    "%cd \"<your-new-directory>\"\n",
    "\n",
    "# Check again\n",
    "%pwd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"Iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then take a look at the first 5 rows of data using the `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to take a look at some random observations from the dataset, we can use the `.sample()` method, providing the number of observations we're interested to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at random 10 observations\n",
    "iris.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s remove the `Id` column. For now, we have no use for it in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = iris.drop(\"Id\", axis=1)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Statistics\n",
    "We can now explore our dataset using descriptive statistics. To find out how many rows and columns the dataset has, we use the `.shape` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that the `iris` dataset has 150 rows and 5 columns.\n",
    "\n",
    "Next, we're going to see how many unique species of *Iris* is available in the dataset. We should have 3 species in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique number of species\n",
    "iris['Species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input / independent variables, we take a look at some statistics (mean, standard deviation, percentile etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check if there are missing values. `isnull()` method is used to detect whether there are missing values in each columns of the dataset. This method returns boolean (`True` / `False`) values. The boolean values are summed up using the `.sum()` method; if it equals to 0, that means there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all columns are 0, therefore we can safely conclude that there are no missing values in this dataset.\n",
    "\n",
    "## Step 4: Correlation Between Variables\n",
    "We are now going to see if there is a correlation between each variable in the data. There are two ways we can do this, either by displaying the correlation values in a dataframe, or through a heatmap.\n",
    "\n",
    "### Method 1\n",
    "We display the correlation coefficients in a dataframe. This is done using the `.corr()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2\n",
    "Display the correlation using a heatmap. \n",
    "+ First, we set the size of our figure using functions from `matplotlib`.\n",
    "+ We create a heatmap using a function from `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(iris.corr(), annot=True, linewidths=.5, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use box and whisker plot to display our input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we plot a scatter matrix as another way to look for any correlation between the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(iris, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Splitting the dataset\n",
    "We split data into training and test sets. These two sets are to be used when we build the classification model.\n",
    "\n",
    "Before we split the dataset into two sets, we first split the dataset into input and target variables, labelled as `X` and `y`, respectively. Remember, we want to classify the observations into Iris species; therefore, the target variable, `y`, will contain the values from the `Species` column. The rest of the columns are input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into input and target variables\n",
    "X = iris.drop(\"Species\", axis = 1)\n",
    "y = iris[\"Species\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several parameters in `train_test_split`.\n",
    "+ `X` and `y`, which are the input and target variables, respectively.\n",
    "+ `train_size` and `test_size`, which specifies the proportion of the dataset to be split into training and test sets. We usually use only either one of these two.\n",
    "+ `random_state`, which ensures that we get the same results everytime we run the code. This is because the function will randomly select the observations to be split. By specifying the argument with any value, we will get the same result no matter how many times we run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we split the dataset where 80% of the observations go to the training set. The function produces an output of a list, which we assign to four variables: `X_train`, `X_test`, `y_train` and `y_test`. As the names suggest, these are the training and test sets of input and target variables.\n",
    "\n",
    "## Step 6: Building the model\n",
    "We are now going to build our classification model. As mentioned, we will use the logistic regression algorithm for classification.\n",
    "\n",
    "We first create an instance of the logistic regression model. We save this as `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fit the model with the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create predictions using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model evaluation\n",
    "We are now going to check the accurayc of our model. We do this using 3 functions: `accuracy_score`, `confusion_matrix` and `classification_report`. We compare the results from the prediction we made earlier (`preds`) with the target variable in the test set(`y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = accuracy_score(y_test, preds)\n",
    "conf_matrix = confusion_matrix(y_test, preds)\n",
    "class_report = classification_report(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines of code are to print out the results of the previous lines of code. Based on the accuracy score, confusion matrix and classification report, what can you conclude from this analysis? Is our model suitable to be sued for classification of *Iris* species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy score: {acc_score}\")\n",
    "print(\"------------------\")\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(\"-------------------\")\n",
    "print(conf_matrix)\n",
    "print(\"----------------------------\")\n",
    "print(\"\\nClassification report:\")\n",
    "print(\"------------------------\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all for data analysis in Python!\n",
    "\n",
    "# References\n",
    "Here are several references for further information about data analysis using Python:\n",
    "+ [Logistic Regression in Python](https://realpython.com/logistic-regression-python/)\n",
    "+ [Scikit-learn documentation](https://scikit-learn.org/stable/index.html) - you can find many tutorials on machine learning models here\n",
    "+ [DataCamp Community Tutorials](https://www.datacamp.com/community/tutorials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
